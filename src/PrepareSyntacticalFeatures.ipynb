{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for extracting Syntactical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, RegexpParser\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "class SyntacticalFeatureExtractorForPT(object):\n",
    "\n",
    "    def __init__(self,fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath):\n",
    "        self.ptFileName = ptFileName\n",
    "        self.ptPath = ptPath\n",
    "        self.ptSavePath = ptSavePath\n",
    "        self.fileType = fileType\n",
    "        self.scalarObjsPath = scalarObjsPath\n",
    "        self.scalarObjs = []\n",
    "        for i in range(1,19):\n",
    "            \n",
    "            readObj = open(os.path.join(scalarObjsPath,f\"scaler_{i}.pkl\"), \"rb\")\n",
    "            self.scalarObjs.append(load(readObj))\n",
    "            readObj.close()\n",
    "            #print(\"Loaded Scalar Obj for Feature \", i)\n",
    "            \n",
    "        self.dataset = torch.load(os.path.join(self.ptPath, ptFileName))\n",
    "        print(\"Loaded %s dataset from %s, number of examples: %d' \",(self.fileType, self.ptPath, len(self.dataset)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def savePtFile(self, finalFeatList):\n",
    "        \n",
    "        torch.save(finalFeatList, os.path.join(self.ptSavePath, self.ptFileName))\n",
    "        \n",
    "        \n",
    "    def extractSyntacticalFeatures(self):\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(self.dataset):\n",
    "            print(\"Working on i\", i+1)\n",
    "            synFeats = SyntacticalProcessorForText(data['src_txt']).finalFeatureExtractor()\n",
    "            scaledSynFeats = [ self.scalarObjs[i].transform(np.array(synFeat).reshape(-1,1)).reshape(1,-1).tolist()[0] for i, synFeat in enumerate(synFeats)]\n",
    "            scaledSynFeats = [ [round(value, 4) for value in feat ] for feat in scaledSynFeats]\n",
    "            self.dataset[i]['sync'] = []\n",
    "            for ind in range(len(data['src_txt'])):\n",
    "                ithSentFeats = [feat[ind] for feat in scaledSynFeats]\n",
    "                self.dataset[i]['sync'].append(ithSentFeats)\n",
    "            self.runValidation(self.dataset[i]['sync'], len(data['src_txt']))\n",
    "                \n",
    "    def runValidation(self, syncData, numOfSent):\n",
    "        #print(\"data type\", type(syncData))\n",
    "        assert np.array(syncData).shape == (numOfSent,18)\n",
    "                  \n",
    "        assert np.any(np.isnan(np.array(syncData))) == False\n",
    "            \n",
    "\n",
    "        \n",
    "class SyntacticalProcessorForText(object):\n",
    "    \n",
    "    def __init__(self, sentList):\n",
    "        \n",
    "        self.sentList = sentList\n",
    "        \n",
    "        self.cleanSentList = self.cleanSentListGen()\n",
    "        \n",
    "        self.originalText = self.originalTextMaker(self.sentList)\n",
    "        \n",
    "        self.originalCleanText = self.originalTextMaker(self.cleanSentList)\n",
    "        \n",
    "        self.countTotalWords = self.countNumberOfWordsInText(self.originalText)\n",
    "        self.countTotalCleanWords = self.countNumberOfWordsInText(self.originalCleanText)\n",
    "        \n",
    "        self.frequencyOfEachWord = self.countFreqOfEachWordInText(self.originalCleanText)\n",
    "        \n",
    "    def convertToTensor(self, listValue):\n",
    "        \n",
    "        return torch.transpose(torch.tensor([listValue]),0,1)\n",
    "    \n",
    "    def sentWordsInTextGen(self, sentList):\n",
    "        \n",
    "        return [ [word for word in sent.split()] for sent in sentList]\n",
    "    \n",
    "    def removePunctuation(self, sentList):\n",
    "        #print([ \"<s>\" + value + \"<e>\" for value in  sentList])\n",
    "        return [re.sub(r'[^\\w\\s]','',sent)  for sent in sentList]\n",
    "    \n",
    "    \n",
    "    def removeStopWords(self, sentList):\n",
    "        \n",
    "        stopWords = list(stopwords.words('english'))\n",
    "        \n",
    "        return [\" \".join(word for word in sent.split() if word not in stopWords) for sent in sentList]\n",
    "        \n",
    "    def cleanSentListGen(self):\n",
    "        \n",
    "        return self.removePunctuation(self.removeStopWords(self.sentList))\n",
    "\n",
    "    \n",
    "    def originalTextMaker(self, sentList):\n",
    "        \n",
    "        return \" \".join(sentList)\n",
    "    \n",
    "    def countFreqOfEachWordInText(self, text):\n",
    "        \n",
    "        words = word_tokenize(text)\n",
    "        fdist = FreqDist(words)\n",
    "        return dict(fdist)        \n",
    "        \n",
    "    \n",
    "    def countNumberOfWordsInText(self, text):\n",
    "        #print(text)\n",
    "        removePunc = re.sub(r'[^\\w\\s]','',text)\n",
    "        return len(removePunc.split())\n",
    "    \n",
    "    def feat1_SumOfWordsFreqInSent(self, sentList):\n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        freqDict = self.countFreqOfEachWordInText(self.originalTextMaker(sentList))\n",
    "        return [ sum([freqDict[word]  for word in sentWords if word in freqDict]) for sentWords in sentWordsList ]\n",
    "    \n",
    "    def feat2_AvgOfWeightedWordsFreqInSent(self, sentList):\n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        #print(\"cc\",self.countTotalCleanWords)\n",
    "        #print([(sentWords,len(sentWords)) for sentWords in sentWordsList])\n",
    "        freqDict = self.countFreqOfEachWordInText(self.originalTextMaker(sentList))\n",
    "        return [ sum([freqDict[word]/self.countNumberOfWordsInText(self.originalTextMaker(sentList)) for word in sentWords if word in freqDict])/len(sentWords) if len(sentWords) != 0 else 0 for sentWords in sentWordsList ]\n",
    "    \n",
    "    def feat3_tfisf(self, sentList):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "    \n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(sentList)\n",
    "        tfIsf = X.toarray()        \n",
    "        \n",
    "        df = pd.DataFrame(tfIsf, columns = vectorizer.get_feature_names())\n",
    "        \n",
    "        sumOfTfIsfOfWordsInSent = [sum([df.iloc[i][word] for word in sentWords if word in list(df.columns)]) for i, sentWords in enumerate(sentWordsList)]\n",
    "        if sumOfTfIsfOfWordsInSent == []:\n",
    "            return []\n",
    "        maxValue = max(sumOfTfIsfOfWordsInSent)\n",
    "        return [value/maxValue for value in sumOfTfIsfOfWordsInSent]\n",
    "    \n",
    "    def feat4_posTags(self, sentList):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        taggedSentList = [nltk.pos_tag(sentWords) for sentWords in sentWordsList]\n",
    "        \n",
    "        #NounTags\n",
    "        NN_Total = [sum([1 for word,tag in taggedSent if 'NN' in tag]) for taggedSent in taggedSentList]\n",
    "\n",
    "        #Verb Tags\n",
    "        VB_Related_Total = [sum([1 for word,tag in taggedSent if 'VB' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Adjective Tags\n",
    "        JJ_Related_Total = [sum([1 for word,tag in taggedSent if 'JJ' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Preposition Tags\n",
    "        IN_Related_Total = [sum([1 for word,tag in taggedSent if tag == 'IN']) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Pronoun Tags\n",
    "        PR_Related_Total = [sum([1 for word,tag in taggedSent if 'PR' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Adverb Tags\n",
    "        RB_Related_Total = [sum([1 for word,tag in taggedSent if 'RB' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #interjection Tags\n",
    "        UH_Related_Total = [sum([1 for word,tag in taggedSent if 'UH' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        NN_feat = [value/sum(NN_Total) if sum(NN_Total) != 0 else 0 for value in NN_Total]\n",
    "        VB_feat = [value/sum(VB_Related_Total) if sum(VB_Related_Total) != 0 else 0 for value in VB_Related_Total]\n",
    "        JJ_feat = [value/sum(JJ_Related_Total) if sum(JJ_Related_Total) != 0 else 0 for value in JJ_Related_Total]\n",
    "        IN_feat = [value/sum(IN_Related_Total) if sum(IN_Related_Total) != 0 else 0 for value in IN_Related_Total]\n",
    "        PR_feat = [value/sum(PR_Related_Total) if sum(PR_Related_Total) != 0 else 0 for value in PR_Related_Total]\n",
    "        RB_feat = [value/sum(RB_Related_Total) if sum(RB_Related_Total) != 0 else 0 for value in RB_Related_Total]\n",
    "        UH_feat = [value/sum(UH_Related_Total) if sum(UH_Related_Total) != 0 else 0 for value in UH_Related_Total]\n",
    "        \n",
    "        return taggedSentList, NN_feat,VB_feat,JJ_feat,IN_feat,PR_feat,RB_feat,UH_feat\n",
    "    \n",
    "    def feat5_SentPositionLabel(self, sentList):\n",
    "        \n",
    "        N = len(sentList) \n",
    "        return [ -1 if (i+1) <= N*0.2 else 1 if (i+1) >= N*0.8 else 0  for i,sent in enumerate(sentList)]\n",
    "    \n",
    "    def feat6_SentPositionWeight(self, sentList):\n",
    "        \n",
    "        N = len(sentList) \n",
    "        return [ 1/(i+1) if (i+1) <= N*0.3 else 1/(N-(i+1)+1) if (i+1) >= N*0.7 else 0  for i,sent in enumerate(sentList)]\n",
    "    \n",
    "\n",
    "    def feat7_SentLengthCharacters(self):\n",
    "        \n",
    "        totalCount = len(self.originalCleanText)\n",
    "    \n",
    "        return [len(sent)/totalCount for sent in self.cleanSentList if totalCount != 0]\n",
    "    \n",
    "    def feat8_SentLengthWords(self):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(self.cleanSentList)\n",
    "        \n",
    "        if sentWordsList != 0:\n",
    "            maxCount = max([ len(sentWords) for sentWords in sentWordsList]) \n",
    "        else:\n",
    "            return []\n",
    "        return [ len(value)/maxCount for value in  sentWordsList if maxCount != 0]\n",
    "    \n",
    "    def feat9_SentLengthStd(self):\n",
    "        import statistics, math\n",
    "        sentWordsList = self.sentWordsInTextGen(self.cleanSentList)\n",
    "        if len(self.cleanSentList) != 0:\n",
    "            avgWordsPerSent = self.countTotalCleanWords/len(self.cleanSentList)\n",
    "            stdWordsPerSent = statistics.stdev([len(sent) for sent in sentWordsList])\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "        return [1/(1+math.log(abs(avgWordsPerSent- abs((avgWordsPerSent - len(sentWords)) /stdWordsPerSent)))) for sentWords in sentWordsList]\n",
    "    \n",
    "    \n",
    "    def feat10_PhrasesInSent(self):\n",
    "        \n",
    "        outputPos = self.feat4_posTags(self.sentList)\n",
    "        posTags = outputPos[0]\n",
    "        chunker = RegexpParser(\"\"\" \n",
    "            NP: {<DT>?<JJ>*<NN>}    #Noun Phrases \n",
    "            P: {<IN>}               #Prepositions \n",
    "            V: {<V.*>}              #Verbs\n",
    "            PP: {<P> <NP>}          #Prepostional Phrases \n",
    "            VP: {<V> <NP|PP>*}      #Verb Phrases \n",
    "                       \"\"\") \n",
    "        \n",
    "        output = [str(chunker.parse(posTag)) for posTag in posTags]\n",
    "        \n",
    "        NP_Count = [ len(re.findall(r'\\(NP',value)) for value in output]\n",
    "        NP_Feat = [value/sum(NP_Count) if value != 0 else 0 for value in NP_Count]\n",
    "        \n",
    "        PP_Count = [ len(re.findall(r'\\(PP',value)) for value in output]\n",
    "        PP_Feat = [value/sum(PP_Count) if value != 0 else 0 for value in PP_Count]\n",
    "        \n",
    "        VP_Count = [ len(re.findall(r'\\(VP',value)) for value in output]\n",
    "        VP_Feat = [value/sum(VP_Count) if value != 0 else 0 for value in VP_Count]\n",
    "    \n",
    "        return NP_Feat, PP_Feat, VP_Feat\n",
    "    \n",
    "    def finalFeatureExtractor(self):\n",
    "        \n",
    "        finalFeat1 = self.feat1_SumOfWordsFreqInSent(self.cleanSentList)\n",
    "        #print(\"finalFeat1 Completed\")\n",
    "        finalFeat2 = self.feat2_AvgOfWeightedWordsFreqInSent(self.cleanSentList)\n",
    "        #print(\"finalFeat2 Completed\")\n",
    "        finalFeat3 = self.feat3_tfisf(self.cleanSentList)\n",
    "        #print(\"finalFeat3 Completed\")\n",
    "        finalFeat_pos_parent = self.feat4_posTags(self.sentList)\n",
    "        #print(\"finalFeat_pos_parent Completed\")\n",
    "        finalFeat4 = finalFeat_pos_parent[1]\n",
    "        #print(\"finalFeat4 Completed\")\n",
    "        finalFeat5 = finalFeat_pos_parent[2]\n",
    "        #print(\"finalFeat5 Completed\")\n",
    "        finalFeat6 = finalFeat_pos_parent[3]\n",
    "        #print(\"finalFeat6 Completed\")\n",
    "        finalFeat7 = finalFeat_pos_parent[4]\n",
    "        #print(\"finalFeat7 Completed\")\n",
    "        finalFeat8 = finalFeat_pos_parent[5]\n",
    "        #print(\"finalFeat8 Completed\")\n",
    "        finalFeat9 = finalFeat_pos_parent[6]\n",
    "        #print(\"finalFeat9 Completed\")\n",
    "        finalFeat10= finalFeat_pos_parent[7]        \n",
    "        #print(\"finalFeat10 Completed\")\n",
    "        finalFeat11 = self.feat5_SentPositionLabel(self.sentList)\n",
    "        #print(\"finalFeat11 Completed\")\n",
    "        finalFeat12 = self.feat6_SentPositionWeight(self.sentList)\n",
    "        #print(\"finalFeat12 Completed\")\n",
    "        finalFeat13 = self.feat7_SentLengthCharacters()\n",
    "        #print(\"finalFeat13 Completed\")\n",
    "        finalFeat14 = self.feat8_SentLengthWords()\n",
    "        #print(\"finalFeat14 Completed\")\n",
    "        finalFeat15 = self.feat9_SentLengthStd()\n",
    "        #print(\"finalFeat15 Completed\")\n",
    "        finalFeat_phrase_parent =  self.feat10_PhrasesInSent()\n",
    "        #print(\"finalFeat_phrase_parent Completed\")\n",
    "        finalFeat16 = finalFeat_phrase_parent[0]\n",
    "        #print(\"finalFeat16 Completed\")\n",
    "        finalFeat17 = finalFeat_phrase_parent[1]\n",
    "        #print(\"finalFeat17 Completed\")\n",
    "        finalFeat18 = finalFeat_phrase_parent[2]\n",
    "        #print(\"finalFeat18 Completed\")\n",
    "        \n",
    "        return [finalFeat1,\\\n",
    "                finalFeat2,\\\n",
    "                finalFeat3,\\\n",
    "                finalFeat4,\\\n",
    "                finalFeat5,\\\n",
    "                finalFeat6,\\\n",
    "                finalFeat7,\\\n",
    "                finalFeat8,\\\n",
    "                finalFeat9,\\\n",
    "                finalFeat10,\\\n",
    "                finalFeat11,\\\n",
    "                finalFeat12,\\\n",
    "                finalFeat13,\\\n",
    "                finalFeat14,\\\n",
    "                finalFeat15,\\\n",
    "                finalFeat16,\\\n",
    "                finalFeat17,\n",
    "                finalFeat18]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling Process\n",
    "Here we extract each syntactical feature separately and write them into separate files. This is only done the training data, as this data saved for fitting and saving a scaling model.<br>\n",
    "This scaling model is used later for processing the PT files for adding scaled syntactical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingPath = os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\bert_data\\\\\")\n",
    "featSavePath = os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\SyntacticalFeatures\\\\\")\n",
    "scalerSavePath =  os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\Scalers\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeature(synClassObj : SyntacticalProcessorForText , featNumber):\n",
    "    \n",
    "    if featNumber == 1:\n",
    "        return synClassObj.feat1_SumOfWordsFreqInSent(synClassObj.cleanSentList)\n",
    "    if featNumber == 2:\n",
    "        return synClassObj.feat2_AvgOfWeightedWordsFreqInSent(synClassObj.cleanSentList)\n",
    "    if featNumber == 3:\n",
    "        return synClassObj.feat3_tfisf(synClassObj.cleanSentList)\n",
    "    if featNumber == 4:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[1]\n",
    "    if featNumber == 5:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[2]\n",
    "    if featNumber == 6:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[3]\n",
    "    if featNumber == 7:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[4]\n",
    "    if featNumber == 8:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[5]\n",
    "    if featNumber == 9:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[6]\n",
    "    if featNumber == 10:\n",
    "        return synClassObj.feat4_posTags(synClassObj.sentList)[7]\n",
    "    if featNumber == 11:\n",
    "        return synClassObj.feat5_SentPositionLabel(synClassObj.sentList)\n",
    "    if featNumber == 12:\n",
    "        return synClassObj.feat6_SentPositionWeight(synClassObj.sentList)\n",
    "    if featNumber == 13:\n",
    "        return synClassObj.feat7_SentLengthCharacters()\n",
    "    if featNumber == 14:\n",
    "        return synClassObj.feat8_SentLengthWords()\n",
    "    if featNumber == 15:\n",
    "        return synClassObj.feat9_SentLengthStd()\n",
    "    if featNumber == 16:\n",
    "        return synClassObj.feat10_PhrasesInSent()[0]\n",
    "    if featNumber == 17:\n",
    "        return synClassObj.feat10_PhrasesInSent()[1]\n",
    "    if featNumber == 18:\n",
    "        return synClassObj.feat10_PhrasesInSent()[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndSaveScalerObj(featPath, featNumber , scalerSavePath):\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    import numpy as np\n",
    "    import ast\n",
    "    from pickle import dump, load\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_{featNumber}_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_{featNumber}_.pkl\"), \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataForFeatures(dataPath, fileType, featNumber, featSavePath, scalerSavePath):\n",
    "    \n",
    "    files = os.listdir(dataPath)\n",
    "    filteredFiles = [ file for file in files if fileType in file]\n",
    "    outputFileObj = open(os.path.join(featSavePath,f\"feat_{featNumber}_.csv\"), \"a\")\n",
    "    \n",
    "    for file in filteredFiles:\n",
    "        if 'git' not in file:\n",
    "            print(file)\n",
    "            loadedTensor = torch.load(os.path.join(dataPath,file))\n",
    "            for ind,articleData in enumerate(loadedTensor):\n",
    "                featList = extractFeature(SyntacticalProcessorForText(articleData['src_txt']), featNumber)\n",
    "                for value in featList:\n",
    "                    outputFileObj.write(f\"{value},\")\n",
    "            \n",
    "    outputFileObj.close()\n",
    "    \n",
    "    fitAndSaveScalerObj(featSavePath, featNumber, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndSaveScalerObjForPos(featPath , scalerSavePath):\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    import numpy as np\n",
    "    import ast\n",
    "    from pickle import dump, load\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_4_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_4_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_5_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_5_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_6_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_6_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_7_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_7_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_8_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_8_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_9_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_9_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_10_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_10_.pkl\"), \"wb\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataForPoSFeatures(dataPath, fileType, featSavePath, scalerSavePath):\n",
    "    import time\n",
    "    files = os.listdir(dataPath)\n",
    "    filteredFiles = [ file for file in files if fileType in file]\n",
    "    outputFileObjs = [open(os.path.join(featSavePath,f\"feat_4_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_5_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_6_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_7_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_8_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_9_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_10_.csv\"), \"a\")]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for file in filteredFiles:\n",
    "        if 'git' not in file:\n",
    "            start = time.time()\n",
    "            print(file)\n",
    "            loadedTensor = torch.load(os.path.join(dataPath,file))\n",
    "            for ind,articleData in enumerate(loadedTensor):\n",
    "                synObj = SyntacticalProcessorForText(articleData['src_txt'])\n",
    "                featList = synObj.feat4_posTags(synObj.sentList)\n",
    "                #print(featList[1:])\n",
    "                for ind, sentFeatList in enumerate(featList[1:]):\n",
    "                    for value in sentFeatList:\n",
    "                        outputFileObjs[ind].write(f\"{value},\")\n",
    "            end = time.time()\n",
    "            print(\"time taken\", (end-start)/60 )\n",
    "    for obj in outputFileObjs:\n",
    "        print(\"asd\")\n",
    "        obj.close()\n",
    "    \n",
    "    fitAndSaveScalerObjForPos(featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndSaveScalerObjForPhrase(featPath , scalerSavePath):\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    import numpy as np\n",
    "    import ast\n",
    "    from pickle import dump, load\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_16_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_16_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_17_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_17_.pkl\"), \"wb\"))\n",
    "    \n",
    "    with open(os.path.join(featPath,f\"feat_18_.csv\"), \"r\") as r:\n",
    "        ll = ast.literal_eval(\"[\" + r.read()[:-1] + \"]\")\n",
    "        r.close()\n",
    "    npList = np.array(ll).reshape(-1,1)\n",
    "    \n",
    "    qt_fit = qt.fit(npList)\n",
    "    dump(qt_fit, open( os.path.join(scalerSavePath,f\"scaler_18_.pkl\"), \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataForPhraseFeatures(dataPath, fileType, featSavePath, scalerSavePath):\n",
    "    import time\n",
    "    files = os.listdir(dataPath)\n",
    "    filteredFiles = [ file for file in files if fileType in file]\n",
    "    outputFileObjs = [open(os.path.join(featSavePath,f\"feat_16_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_17_.csv\"), \"a\"),\\\n",
    "                      open(os.path.join(featSavePath,f\"feat_18_.csv\"), \"a\")]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for file in filteredFiles:\n",
    "        if 'git' not in file:\n",
    "            start = time.time()\n",
    "            print(file)\n",
    "            loadedTensor = torch.load(os.path.join(dataPath,file))\n",
    "            for ind,articleData in enumerate(loadedTensor):\n",
    "                synObj = SyntacticalProcessorForText(articleData['src_txt'])\n",
    "                featList = synObj.feat10_PhrasesInSent()\n",
    "                \n",
    "                for ind, sentFeatList in enumerate(featList):\n",
    "                    for value in sentFeatList:\n",
    "                        outputFileObjs[ind].write(f\"{value},\")\n",
    "                \n",
    "            end = time.time()\n",
    "            print(\"time taken\", (end-start)/60 )\n",
    "    for obj in outputFileObjs:\n",
    "        print(\"asd\")\n",
    "        obj.close()\n",
    "    \n",
    "    fitAndSaveScalerObjForPhrase(featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 1, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 2, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 3, featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepareDataForPoSFeatures(trainingPath, \"train\", featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 11, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 12, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 13, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 14, featSavePath, scalerSavePath)\n",
    "\n",
    "prepareDataForFeatures(trainingPath, \"train\", 15, featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepareDataForPhraseFeatures(trainingPath, \"train\", featSavePath, scalerSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process PT Files to add scaled Syntactical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileType, ptPath, ptSavePath, scalarObjsPath = \"train\",\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\bert_data\\\\\"),\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\bert_data_sync\\\\\"),\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\Scalers\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractSynFeatsAndSavePtFiles(fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath):\n",
    "\n",
    "\n",
    "    obj = SyntacticalFeatureExtractorForPT(fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath)\n",
    "    obj.extractSyntacticalFeatures()\n",
    "    obj.savePtFile(obj.dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalLoader(fileType, ptPath, ptSavePath, scalarObjsPath):\n",
    "\n",
    "    allFiles = [fileName for fileName in os.listdir(ptPath) if fileType in fileName]\n",
    "\n",
    "\n",
    "    for i, fileName in enumerate(allFiles):\n",
    "        \n",
    "            print(\"Working on file\", fileName)\n",
    "            extractSynFeatsAndSavePtFiles(fileType, fileName, ptPath, ptSavePath, scalarObjsPath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalLoader(fileType, ptPath, ptSavePath, scalarObjsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca7d09a1a4271beb368e0eee4e97c0723f0fb16c9145fcfc98611f23c52a0ba7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
