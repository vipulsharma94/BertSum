{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, RegexpParser\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SyntacticalFeatureExtractorForPT(object):\n",
    "\n",
    "    def __init__(self,fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath):\n",
    "        self.ptFileName = ptFileName\n",
    "        self.ptPath = ptPath\n",
    "        self.ptSavePath = ptSavePath\n",
    "        self.fileType = fileType\n",
    "        self.scalarObjsPath = scalarObjsPath\n",
    "        self.scalarObjs = []\n",
    "        for i in range(1,19):\n",
    "            \n",
    "            readObj = open(os.path.join(scalarObjsPath,f\"scaler_{i}.pkl\"), \"rb\")\n",
    "            self.scalarObjs.append(load(readObj))\n",
    "            readObj.close()\n",
    "            #print(\"Loaded Scalar Obj for Feature \", i)\n",
    "            \n",
    "        self.dataset = torch.load(os.path.join(self.ptPath, ptFileName))\n",
    "        print(\"Loaded %s dataset from %s, number of examples: %d' \",(self.fileType, self.ptPath, len(self.dataset)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def savePtFile(self, finalFeatList):\n",
    "        \n",
    "        torch.save(finalFeatList, os.path.join(self.ptSavePath, self.ptFileName))\n",
    "        \n",
    "        \n",
    "    def extractSyntacticalFeatures(self):\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(self.dataset):\n",
    "            print(\"Working on i\", i+1)\n",
    "            synFeats = SyntacticalProcessorForText(data['src_txt']).finalFeatureExtractor()\n",
    "            scaledSynFeats = [ self.scalarObjs[i].transform(np.array(synFeat).reshape(-1,1)).reshape(1,-1).tolist()[0] for i, synFeat in enumerate(synFeats)]\n",
    "            scaledSynFeats = [ [round(value, 4) for value in feat ] for feat in scaledSynFeats]\n",
    "            self.dataset[i]['sync'] = []\n",
    "            for ind in range(len(data['src_txt'])):\n",
    "                ithSentFeats = [feat[ind] for feat in scaledSynFeats]\n",
    "                self.dataset[i]['sync'].append(ithSentFeats)\n",
    "            self.runValidation(self.dataset[i]['sync'], len(data['src_txt']))\n",
    "                \n",
    "    def runValidation(self, syncData, numOfSent):\n",
    "        #print(\"data type\", type(syncData))\n",
    "        assert np.array(syncData).shape == (numOfSent,18)\n",
    "                  \n",
    "        assert np.any(np.isnan(np.array(syncData))) == False\n",
    "            \n",
    "\n",
    "        \n",
    "class SyntacticalProcessorForText(object):\n",
    "    \n",
    "    def __init__(self, sentList):\n",
    "        \n",
    "        self.sentList = sentList\n",
    "        \n",
    "        self.cleanSentList = self.cleanSentListGen()\n",
    "        \n",
    "        self.originalText = self.originalTextMaker(self.sentList)\n",
    "        \n",
    "        self.originalCleanText = self.originalTextMaker(self.cleanSentList)\n",
    "        \n",
    "        self.countTotalWords = self.countNumberOfWordsInText(self.originalText)\n",
    "        self.countTotalCleanWords = self.countNumberOfWordsInText(self.originalCleanText)\n",
    "        \n",
    "        self.frequencyOfEachWord = self.countFreqOfEachWordInText(self.originalCleanText)\n",
    "        \n",
    "    def convertToTensor(self, listValue):\n",
    "        \n",
    "        return torch.transpose(torch.tensor([listValue]),0,1)\n",
    "    \n",
    "    def sentWordsInTextGen(self, sentList):\n",
    "        \n",
    "        return [ [word for word in sent.split()] for sent in sentList]\n",
    "    \n",
    "    def removePunctuation(self, sentList):\n",
    "        #print([ \"<s>\" + value + \"<e>\" for value in  sentList])\n",
    "        return [re.sub(r'[^\\w\\s]','',sent)  for sent in sentList]\n",
    "    \n",
    "    \n",
    "    def removeStopWords(self, sentList):\n",
    "        \n",
    "        stopWords = list(stopwords.words('english'))\n",
    "        \n",
    "        return [\" \".join(word for word in sent.split() if word not in stopWords) for sent in sentList]\n",
    "        \n",
    "    def cleanSentListGen(self):\n",
    "        \n",
    "        return self.removePunctuation(self.removeStopWords(self.sentList))\n",
    "\n",
    "    \n",
    "    def originalTextMaker(self, sentList):\n",
    "        \n",
    "        return \" \".join(sentList)\n",
    "    \n",
    "    def countFreqOfEachWordInText(self, text):\n",
    "        \n",
    "        words = word_tokenize(text)\n",
    "        fdist = FreqDist(words)\n",
    "        return dict(fdist)        \n",
    "        \n",
    "    \n",
    "    def countNumberOfWordsInText(self, text):\n",
    "        #print(text)\n",
    "        removePunc = re.sub(r'[^\\w\\s]','',text)\n",
    "        return len(removePunc.split())\n",
    "    \n",
    "    def feat1_SumOfWordsFreqInSent(self, sentList):\n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        freqDict = self.countFreqOfEachWordInText(self.originalTextMaker(sentList))\n",
    "        return [ sum([freqDict[word]  for word in sentWords if word in freqDict]) for sentWords in sentWordsList ]\n",
    "    \n",
    "    def feat2_AvgOfWeightedWordsFreqInSent(self, sentList):\n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        #print(\"cc\",self.countTotalCleanWords)\n",
    "        #print([(sentWords,len(sentWords)) for sentWords in sentWordsList])\n",
    "        freqDict = self.countFreqOfEachWordInText(self.originalTextMaker(sentList))\n",
    "        return [ sum([freqDict[word]/self.countNumberOfWordsInText(self.originalTextMaker(sentList)) for word in sentWords if word in freqDict])/len(sentWords) if len(sentWords) != 0 else 0 for sentWords in sentWordsList ]\n",
    "    \n",
    "    def feat3_tfisf(self, sentList):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "    \n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(sentList)\n",
    "        tfIsf = X.toarray()        \n",
    "        \n",
    "        df = pd.DataFrame(tfIsf, columns = vectorizer.get_feature_names())\n",
    "        \n",
    "        sumOfTfIsfOfWordsInSent = [sum([df.iloc[i][word] for word in sentWords if word in list(df.columns)]) for i, sentWords in enumerate(sentWordsList)]\n",
    "        if sumOfTfIsfOfWordsInSent == []:\n",
    "            return []\n",
    "        maxValue = max(sumOfTfIsfOfWordsInSent)\n",
    "        return [value/maxValue for value in sumOfTfIsfOfWordsInSent]\n",
    "    \n",
    "    def feat4_posTags(self, sentList):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(sentList)\n",
    "        taggedSentList = [nltk.pos_tag(sentWords) for sentWords in sentWordsList]\n",
    "        \n",
    "        #NounTags\n",
    "        NN_Total = [sum([1 for word,tag in taggedSent if 'NN' in tag]) for taggedSent in taggedSentList]\n",
    "\n",
    "        #Verb Tags\n",
    "        VB_Related_Total = [sum([1 for word,tag in taggedSent if 'VB' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Adjective Tags\n",
    "        JJ_Related_Total = [sum([1 for word,tag in taggedSent if 'JJ' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Preposition Tags\n",
    "        IN_Related_Total = [sum([1 for word,tag in taggedSent if tag == 'IN']) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Pronoun Tags\n",
    "        PR_Related_Total = [sum([1 for word,tag in taggedSent if 'PR' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #Adverb Tags\n",
    "        RB_Related_Total = [sum([1 for word,tag in taggedSent if 'RB' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        #interjection Tags\n",
    "        UH_Related_Total = [sum([1 for word,tag in taggedSent if 'UH' in tag]) for taggedSent in taggedSentList]\n",
    "        \n",
    "        NN_feat = [value/sum(NN_Total) if sum(NN_Total) != 0 else 0 for value in NN_Total]\n",
    "        VB_feat = [value/sum(VB_Related_Total) if sum(VB_Related_Total) != 0 else 0 for value in VB_Related_Total]\n",
    "        JJ_feat = [value/sum(JJ_Related_Total) if sum(JJ_Related_Total) != 0 else 0 for value in JJ_Related_Total]\n",
    "        IN_feat = [value/sum(IN_Related_Total) if sum(IN_Related_Total) != 0 else 0 for value in IN_Related_Total]\n",
    "        PR_feat = [value/sum(PR_Related_Total) if sum(PR_Related_Total) != 0 else 0 for value in PR_Related_Total]\n",
    "        RB_feat = [value/sum(RB_Related_Total) if sum(RB_Related_Total) != 0 else 0 for value in RB_Related_Total]\n",
    "        UH_feat = [value/sum(UH_Related_Total) if sum(UH_Related_Total) != 0 else 0 for value in UH_Related_Total]\n",
    "        \n",
    "        return taggedSentList, NN_feat,VB_feat,JJ_feat,IN_feat,PR_feat,RB_feat,UH_feat\n",
    "    \n",
    "    def feat5_SentPositionLabel(self, sentList):\n",
    "        \n",
    "        N = len(sentList) \n",
    "        return [ -1 if (i+1) <= N*0.2 else 1 if (i+1) >= N*0.8 else 0  for i,sent in enumerate(sentList)]\n",
    "    \n",
    "    def feat6_SentPositionWeight(self, sentList):\n",
    "        \n",
    "        N = len(sentList) \n",
    "        return [ 1/(i+1) if (i+1) <= N*0.3 else 1/(N-(i+1)+1) if (i+1) >= N*0.7 else 0  for i,sent in enumerate(sentList)]\n",
    "    \n",
    "\n",
    "    def feat7_SentLengthCharacters(self):\n",
    "        \n",
    "        totalCount = len(self.originalCleanText)\n",
    "    \n",
    "        return [len(sent)/totalCount for sent in self.cleanSentList if totalCount != 0]\n",
    "    \n",
    "    def feat8_SentLengthWords(self):\n",
    "        \n",
    "        sentWordsList = self.sentWordsInTextGen(self.cleanSentList)\n",
    "        \n",
    "        if sentWordsList != 0:\n",
    "            maxCount = max([ len(sentWords) for sentWords in sentWordsList]) \n",
    "        else:\n",
    "            return []\n",
    "        return [ len(value)/maxCount for value in  sentWordsList if maxCount != 0]\n",
    "    \n",
    "    def feat9_SentLengthStd(self):\n",
    "        import statistics, math\n",
    "        sentWordsList = self.sentWordsInTextGen(self.cleanSentList)\n",
    "        if len(self.cleanSentList) != 0:\n",
    "            avgWordsPerSent = self.countTotalCleanWords/len(self.cleanSentList)\n",
    "            stdWordsPerSent = statistics.stdev([len(sent) for sent in sentWordsList])\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "        return [1/(1+math.log(abs(avgWordsPerSent- abs((avgWordsPerSent - len(sentWords)) /stdWordsPerSent)))) for sentWords in sentWordsList]\n",
    "    \n",
    "    \n",
    "    def feat10_PhrasesInSent(self):\n",
    "        \n",
    "        outputPos = self.feat4_posTags(self.sentList)\n",
    "        posTags = outputPos[0]\n",
    "        chunker = RegexpParser(\"\"\" \n",
    "            NP: {<DT>?<JJ>*<NN>}    #Noun Phrases \n",
    "            P: {<IN>}               #Prepositions \n",
    "            V: {<V.*>}              #Verbs\n",
    "            PP: {<P> <NP>}          #Prepostional Phrases \n",
    "            VP: {<V> <NP|PP>*}      #Verb Phrases \n",
    "                       \"\"\") \n",
    "        \n",
    "        output = [str(chunker.parse(posTag)) for posTag in posTags]\n",
    "        \n",
    "        NP_Count = [ len(re.findall(r'\\(NP',value)) for value in output]\n",
    "        NP_Feat = [value/sum(NP_Count) if value != 0 else 0 for value in NP_Count]\n",
    "        \n",
    "        PP_Count = [ len(re.findall(r'\\(PP',value)) for value in output]\n",
    "        PP_Feat = [value/sum(PP_Count) if value != 0 else 0 for value in PP_Count]\n",
    "        \n",
    "        VP_Count = [ len(re.findall(r'\\(VP',value)) for value in output]\n",
    "        VP_Feat = [value/sum(VP_Count) if value != 0 else 0 for value in VP_Count]\n",
    "    \n",
    "        return NP_Feat, PP_Feat, VP_Feat\n",
    "    \n",
    "    def finalFeatureExtractor(self):\n",
    "        \n",
    "        finalFeat1 = self.feat1_SumOfWordsFreqInSent(self.cleanSentList)\n",
    "        #print(\"finalFeat1 Completed\")\n",
    "        finalFeat2 = self.feat2_AvgOfWeightedWordsFreqInSent(self.cleanSentList)\n",
    "        #print(\"finalFeat2 Completed\")\n",
    "        finalFeat3 = self.feat3_tfisf(self.cleanSentList)\n",
    "        #print(\"finalFeat3 Completed\")\n",
    "        finalFeat_pos_parent = self.feat4_posTags(self.sentList)\n",
    "        #print(\"finalFeat_pos_parent Completed\")\n",
    "        finalFeat4 = finalFeat_pos_parent[1]\n",
    "        #print(\"finalFeat4 Completed\")\n",
    "        finalFeat5 = finalFeat_pos_parent[2]\n",
    "        #print(\"finalFeat5 Completed\")\n",
    "        finalFeat6 = finalFeat_pos_parent[3]\n",
    "        #print(\"finalFeat6 Completed\")\n",
    "        finalFeat7 = finalFeat_pos_parent[4]\n",
    "        #print(\"finalFeat7 Completed\")\n",
    "        finalFeat8 = finalFeat_pos_parent[5]\n",
    "        #print(\"finalFeat8 Completed\")\n",
    "        finalFeat9 = finalFeat_pos_parent[6]\n",
    "        #print(\"finalFeat9 Completed\")\n",
    "        finalFeat10= finalFeat_pos_parent[7]        \n",
    "        #print(\"finalFeat10 Completed\")\n",
    "        finalFeat11 = self.feat5_SentPositionLabel(self.sentList)\n",
    "        #print(\"finalFeat11 Completed\")\n",
    "        finalFeat12 = self.feat6_SentPositionWeight(self.sentList)\n",
    "        #print(\"finalFeat12 Completed\")\n",
    "        finalFeat13 = self.feat7_SentLengthCharacters()\n",
    "        #print(\"finalFeat13 Completed\")\n",
    "        finalFeat14 = self.feat8_SentLengthWords()\n",
    "        #print(\"finalFeat14 Completed\")\n",
    "        finalFeat15 = self.feat9_SentLengthStd()\n",
    "        #print(\"finalFeat15 Completed\")\n",
    "        finalFeat_phrase_parent =  self.feat10_PhrasesInSent()\n",
    "        #print(\"finalFeat_phrase_parent Completed\")\n",
    "        finalFeat16 = finalFeat_phrase_parent[0]\n",
    "        #print(\"finalFeat16 Completed\")\n",
    "        finalFeat17 = finalFeat_phrase_parent[1]\n",
    "        #print(\"finalFeat17 Completed\")\n",
    "        finalFeat18 = finalFeat_phrase_parent[2]\n",
    "        #print(\"finalFeat18 Completed\")\n",
    "        \n",
    "        return [finalFeat1,\\\n",
    "                finalFeat2,\\\n",
    "                finalFeat3,\\\n",
    "                finalFeat4,\\\n",
    "                finalFeat5,\\\n",
    "                finalFeat6,\\\n",
    "                finalFeat7,\\\n",
    "                finalFeat8,\\\n",
    "                finalFeat9,\\\n",
    "                finalFeat10,\\\n",
    "                finalFeat11,\\\n",
    "                finalFeat12,\\\n",
    "                finalFeat13,\\\n",
    "                finalFeat14,\\\n",
    "                finalFeat15,\\\n",
    "                finalFeat16,\\\n",
    "                finalFeat17,\n",
    "                finalFeat18]\n",
    "    \n",
    "    def finalFeatureExtractorToTensor(self):\n",
    "       \n",
    "        return torch.cat((self.convertToTensor(self.feat1_SumOfWordsFreqInSent(self.cleanSentList)),\\\n",
    "                          self.convertToTensor(self.feat2_AvgOfWeightedWordsFreqInSent(self.cleanSentList)),\\\n",
    "                          self.convertToTensor(self.feat3_tfisf(self.cleanSentList)),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[1]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[2]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[3]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[4]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[5]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[6]),\\\n",
    "                          self.convertToTensor(self.feat4_posTags(self.sentList)[7]),\\\n",
    "                          self.convertToTensor(self.feat5_SentPositionLabel(self.sentList)),\\\n",
    "                          self.convertToTensor(self.feat6_SentPositionWeight(self.sentList)),\\\n",
    "                          self.convertToTensor(self.feat7_SentLengthCharacters()),\\\n",
    "                          self.convertToTensor(self.feat8_SentLengthWords()),\\\n",
    "                          self.convertToTensor(self.feat9_SentLengthStd()),\\\n",
    "                          self.convertToTensor(self.feat10_PhrasesInSent()[0]),\\\n",
    "                          self.convertToTensor(self.feat10_PhrasesInSent()[1]),\\\n",
    "                          self.convertToTensor(self.feat10_PhrasesInSent()[2])),dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileType, ptPath, ptSavePath, scalarObjsPath = \"train\",\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\bert_data\\\\\"),\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\bert_data_sync\\\\\"),\\\n",
    "                                                os.path.join(\"F:\\\\Learning\\\\LJMU\\\\finalProject\\\\BertSum\\\\Scalers\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractSynFeatsAndSavePtFiles(fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath):\n",
    "\n",
    "\n",
    "    obj = SyntacticalFeatureExtractorForPT(fileType, ptFileName, ptPath, ptSavePath, scalarObjsPath)\n",
    "    obj.extractSyntacticalFeatures()\n",
    "    obj.savePtFile(obj.dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalLoader(fileType, ptPath, ptSavePath, scalarObjsPath):\n",
    "\n",
    "    allFiles = [fileName for fileName in os.listdir(ptPath) if fileType in fileName]\n",
    "\n",
    "\n",
    "    for i, fileName in enumerate(allFiles):\n",
    "        \n",
    "        if i < 10 :\n",
    "            print(\"Working on file\", fileName)\n",
    "            extractSynFeatsAndSavePtFiles(fileType, fileName, ptPath, ptSavePath, scalarObjsPath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalLoader(fileType, ptPath, ptSavePath, scalarObjsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
