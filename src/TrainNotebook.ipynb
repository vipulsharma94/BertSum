{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 12.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: boto3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.20.19)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.19.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.10.2)\n",
            "Requirement already satisfied: regex in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2022.4.24)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.19 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (1.23.19)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.19->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.19->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.5)\r\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from tensorboardX) (1.19.0)\r\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from tensorboardX) (3.20.0)\r\n",
            "Requirement already satisfied: six in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from tensorboardX) (1.16.0)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: multiprocess in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.70.12.2)\r\n",
            "Requirement already satisfied: dill>=0.3.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from multiprocess) (0.3.4)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install multiprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gather": {
          "logged": 1653689326966
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Install pyrouge using top solution on\n",
        "https://stackoverflow.com/questions/45894212/installing-pyrouge-gets-error-in-ubuntu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def createFolders(Name):\n",
        "    try:\n",
        "\n",
        "        os.mkdir(f\"../{Name}\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "\n",
        "        os.mkdir(f\"../{Name}/logs\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "\n",
        "        os.mkdir(f\"../{Name}/results\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "\n",
        "        os.mkdir(f\"../{Name}/models\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "\n",
        "        os.mkdir(f\"../{Name}/temp\")\n",
        "    except:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linePrinter(modelName, synFeats, heads = 8 ,  warmup = \"5000\",layers = \"2\" ,trainSteps = \"25000\" ,lr = \"2e-3\"):\n",
        "    print(f\"!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../{modelName}/models/bert_transformer -lr {lr} -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps {trainSteps} -accum_count 2 -log_file ../{modelName}/logs/bert_transformer -use_interval true -warmup_steps {warmup} -ff_size 2048 -inter_layers {layers} -heads {heads} -temp_dir ../{modelName}/temp -syntFeatIndexList {synFeats}\")\n",
        "    print(f\"!python train.py -mode validate -bert_data_path ../bert_data_sync/cnndm  -model_path ../{modelName}/models/bert_transformer  -visible_gpus 0  -gpu_ranks 0 -batch_size 30000  -log_file ../{modelName}/logs/bert_transformer_test  -result_path ../{modelName}/results/cnndm -test_all -block_trigram true -temp_dir ../{modelName}/temp -syntFeatIndexList {synFeats}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Final Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 1\n",
        "# Train params :- 8 heads, 25000 steps, 2 inter_layers, 2e-3 lr, and 5000 warmup_steps. \n",
        "#For Running this we now need to comment a few lines encoder.py\n",
        "createFolders(\"Exp1_model_SOA\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp1_model_SOA/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp1_model_SOA/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 8 -temp_dir ../Exp1_model_SOA/temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 2\n",
        "# Train params :- 3 heads, 25000 steps, 2 inter_layers, 5000 warmup_steps, 2e-3 lr, and [0,1,2] syntFeatIndexList.\n",
        "\n",
        "createFolders(\"Exp2_model_wordsFeat\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp2_model_wordsFeat/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp2_model_wordsFeat/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 3 -temp_dir ../Exp2_model_wordsFeat/temp -syntFeatIndexList \"0,1,2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 3\n",
        "# Train params :- 9 heads, 25000 steps, 2 inter_layers, 5000 warmup_steps, 2e-3 lr, and [9,10,11,12,13,14] syntFeatIndexList.\n",
        "\n",
        "createFolders(\"Exp3_model_sentPosLen\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp3_model_sentPosLen/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp3_model_sentPosLen/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 9 -temp_dir ../Exp3_model_sentPosLen/temp -syntFeatIndexList \"9,10,11,12,13,14\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 4\n",
        "#Train params :- 7 heads, 25000 steps, 2 inter_layers, 5000 warmup_steps, 2e-3 lr, and [3,4,5,6,7,8,15,16,17] syntFeatIndexList.\n",
        "\n",
        "createFolders(\"Exp4_model_posAndPhrase\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp4_model_posAndPhrase/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp4_model_posAndPhrase/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 7 -temp_dir ../Exp4_model_posAndPhrase/temp -syntFeatIndexList \"3,4,5,6,7,8,15,16,17\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 5\n",
        "#Train params :- 8 heads, 25000 steps, 2 inter_layers, 5000 warmup_steps, 2e-3 lr, and  [0,1,2,10,11,12,13,14] syntFeatIndexList.\n",
        "\n",
        "createFolders(\"Exp5_model_wordsAndSent\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp5_model_wordsAndSent/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp5_model_wordsAndSent/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 8 -temp_dir ../Exp5_model_wordsAndSent/temp -syntFeatIndexList \"0,1,2,10,11,12,13,14\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 6\n",
        "#Train params :- 6 heads, 25000 steps, 2 inter_layers, 2e-3 lr, and 5000 warmup_steps.\n",
        "\n",
        "createFolders(\"Exp6_model_allFeats\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp6_model_allFeats/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp6_model_allFeats/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 6 -temp_dir ../Exp6_model_allFeats/temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 7\n",
        "#Train params :- 14 heads, 25000 steps, 2 inter_layers, 2e-3 lr, and 5000 warmup_steps.\n",
        "\n",
        "createFolders(\"Exp7_model_increasedHeads\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp7_model_increasedHeads/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp7_model_increasedHeads/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 14 -temp_dir ../Exp7_model_increasedHeads/temp -syntFeatIndexList \"1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 8\n",
        "#Train params :- 6 heads, 35000 steps, 2 inter_layers, 2e-4 lr. And 5000 warmup_steps\n",
        "\n",
        "createFolders(\"Exp8_model_LowerLR\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp8_model_lowerLR/models/bert_transformer -lr 2e-4 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 35000 -accum_count 2 -log_file ../Exp8_model_lowerLR/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 6 -temp_dir ../Exp8_model_lowerLR/temp -syntFeatIndexList \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 9\n",
        "#Train params :- 6 heads, 25000 steps, 2 inter_layers, 2e-2 lr. And 3000 warmup_steps.\n",
        "\n",
        "createFolders(\"Exp9_model_higherLR\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp9_model_higherLR/models/bert_transformer -lr 2e-2 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp9_model_higherLR/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 2 -heads 6 -temp_dir ../Exp9_model_higherLR/temp -syntFeatIndexList \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Experiment 10\n",
        "#Train params :- 6 heads, 25000 steps, 3 inter_layers, 2e-3 lr. And 5000 warmup_steps.\n",
        "\n",
        "createFolders(\"Exp10_model_extraLayer\")\n",
        "!python train.py -mode train -encoder transformer -dropout 0.1 -bert_data_path ../bert_data_sync/cnndm -model_path ../Exp10_model_extraLayer/models/bert_transformer -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 25000 -accum_count 2 -log_file ../Exp10_model_extraLayer/logs/bert_transformer -use_interval true -warmup_steps 5000 -ff_size 2048 -inter_layers 3 -heads 6 -temp_dir ../Exp10_model_extraLayer/temp -syntFeatIndexList \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ca7d09a1a4271beb368e0eee4e97c0723f0fb16c9145fcfc98611f23c52a0ba7"
    },
    "kernel_info": {
      "name": "azureml_py38_pt_tf"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
